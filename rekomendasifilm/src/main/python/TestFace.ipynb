{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782bb08e-9b80-4253-9063-3afd6df1e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "from tensorflow.keras.applications import VGG16, InceptionResNetV2\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99842346-9d4d-43bd-90f0-f856ef2f8166",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"DatasetEmotion/train\" \n",
    "test_dir = \"DatasetEmotion/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b965c67f-e76b-4360-9a4a-1afa572d00a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "model= tf.keras.models.Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(48, 48,1)))\n",
    "model.add(Conv2D(64,(3,3), padding='same', activation='relu' ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(5,5), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "    \n",
    "model.add(Conv2D(512,(3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(512,(3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(256,activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "    \n",
    "model.add(Dense(512,activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c36c190b-1939-415e-8e65-5e41d01c9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99db96e3-4bd1-4077-9fd8-0585f56480df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(width_shift_range = 0.1,\n",
    "                                         height_shift_range = 0.1,\n",
    "                                         horizontal_flip = True,\n",
    "                                         rescale = 1./255,\n",
    "                                         validation_split = 0.2\n",
    "                                        )\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                         validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d5b69049-e3f8-454d-835b-3b06802091be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n",
      "Found 1432 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(directory = train_dir,\n",
    "                                                    target_size = (img_size,img_size),\n",
    "                                                    batch_size = 64,\n",
    "                                                    color_mode = \"grayscale\",\n",
    "                                                    class_mode = \"categorical\",\n",
    "                                                    subset = \"training\"\n",
    "                                                   )\n",
    "validation_generator = validation_datagen.flow_from_directory( directory = test_dir,\n",
    "                                                              target_size = (img_size,img_size),\n",
    "                                                              batch_size = 64,\n",
    "                                                              color_mode = \"grayscale\",\n",
    "                                                              class_mode = \"categorical\",\n",
    "                                                              subset = \"validation\"\n",
    "                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7447a373-46d8-4809-8db3-267aeb7491ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = Adam(learning_rate=0.001), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d4795b4-b450-46e1-905c-91acf8e26a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 45\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b949405-8ff6-418a-8627-f19335a8db19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 1s/step - accuracy: 0.2281 - loss: 2.0982 - val_accuracy: 0.2814 - val_loss: 1.8445\n",
      "Epoch 2/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 854ms/step - accuracy: 0.3577 - loss: 1.6826 - val_accuracy: 0.4078 - val_loss: 1.5701\n",
      "Epoch 3/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 614ms/step - accuracy: 0.4488 - loss: 1.4264 - val_accuracy: 0.5021 - val_loss: 1.3149\n",
      "Epoch 4/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 621ms/step - accuracy: 0.4971 - loss: 1.3126 - val_accuracy: 0.5405 - val_loss: 1.2329\n",
      "Epoch 5/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 603ms/step - accuracy: 0.5293 - loss: 1.2266 - val_accuracy: 0.5796 - val_loss: 1.1340\n",
      "Epoch 6/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 624ms/step - accuracy: 0.5503 - loss: 1.1904 - val_accuracy: 0.5573 - val_loss: 1.1508\n",
      "Epoch 7/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 624ms/step - accuracy: 0.5632 - loss: 1.1550 - val_accuracy: 0.5803 - val_loss: 1.1001\n",
      "Epoch 8/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 624ms/step - accuracy: 0.5698 - loss: 1.1205 - val_accuracy: 0.5950 - val_loss: 1.0887\n",
      "Epoch 9/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 516ms/step - accuracy: 0.5771 - loss: 1.1085 - val_accuracy: 0.6020 - val_loss: 1.0409\n",
      "Epoch 10/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 486ms/step - accuracy: 0.5973 - loss: 1.0532 - val_accuracy: 0.5761 - val_loss: 1.1083\n",
      "Epoch 11/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 620ms/step - accuracy: 0.6031 - loss: 1.0592 - val_accuracy: 0.6152 - val_loss: 1.0388\n",
      "Epoch 12/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 621ms/step - accuracy: 0.6113 - loss: 1.0221 - val_accuracy: 0.6145 - val_loss: 1.0396\n",
      "Epoch 13/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 640ms/step - accuracy: 0.6098 - loss: 1.0280 - val_accuracy: 0.6243 - val_loss: 1.0220\n",
      "Epoch 14/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 589ms/step - accuracy: 0.6217 - loss: 0.9967 - val_accuracy: 0.5601 - val_loss: 1.2090\n",
      "Epoch 15/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 565ms/step - accuracy: 0.6233 - loss: 0.9846 - val_accuracy: 0.6187 - val_loss: 1.0021\n",
      "Epoch 16/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 652ms/step - accuracy: 0.6316 - loss: 0.9847 - val_accuracy: 0.6362 - val_loss: 0.9961\n",
      "Epoch 17/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 648ms/step - accuracy: 0.6345 - loss: 0.9589 - val_accuracy: 0.6194 - val_loss: 0.9927\n",
      "Epoch 18/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 647ms/step - accuracy: 0.6428 - loss: 0.9468 - val_accuracy: 0.6222 - val_loss: 1.0633\n",
      "Epoch 19/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 636ms/step - accuracy: 0.6428 - loss: 0.9458 - val_accuracy: 0.6236 - val_loss: 1.0415\n",
      "Epoch 20/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 640ms/step - accuracy: 0.6575 - loss: 0.9078 - val_accuracy: 0.6236 - val_loss: 1.0282\n",
      "Epoch 21/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 651ms/step - accuracy: 0.6510 - loss: 0.9103 - val_accuracy: 0.6397 - val_loss: 0.9787\n",
      "Epoch 22/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 634ms/step - accuracy: 0.6587 - loss: 0.9084 - val_accuracy: 0.6166 - val_loss: 1.0184\n",
      "Epoch 23/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 632ms/step - accuracy: 0.6628 - loss: 0.9048 - val_accuracy: 0.6446 - val_loss: 0.9644\n",
      "Epoch 24/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 578ms/step - accuracy: 0.6705 - loss: 0.8901 - val_accuracy: 0.6264 - val_loss: 1.0391\n",
      "Epoch 25/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 456ms/step - accuracy: 0.6754 - loss: 0.8678 - val_accuracy: 0.6292 - val_loss: 1.0118\n",
      "Epoch 26/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 476ms/step - accuracy: 0.6720 - loss: 0.8698 - val_accuracy: 0.6243 - val_loss: 1.0554\n",
      "Epoch 27/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 418ms/step - accuracy: 0.6755 - loss: 0.8631 - val_accuracy: 0.6543 - val_loss: 0.9728\n",
      "Epoch 28/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 403ms/step - accuracy: 0.6833 - loss: 0.8395 - val_accuracy: 0.6453 - val_loss: 1.0215\n",
      "Epoch 29/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 428ms/step - accuracy: 0.6862 - loss: 0.8302 - val_accuracy: 0.6383 - val_loss: 0.9676\n",
      "Epoch 30/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 476ms/step - accuracy: 0.6867 - loss: 0.8361 - val_accuracy: 0.6411 - val_loss: 0.9730\n",
      "Epoch 31/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 471ms/step - accuracy: 0.6827 - loss: 0.8287 - val_accuracy: 0.6425 - val_loss: 0.9726\n",
      "Epoch 32/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 473ms/step - accuracy: 0.6974 - loss: 0.8173 - val_accuracy: 0.6592 - val_loss: 0.9726\n",
      "Epoch 33/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 472ms/step - accuracy: 0.6953 - loss: 0.8097 - val_accuracy: 0.6536 - val_loss: 0.9529\n",
      "Epoch 34/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 481ms/step - accuracy: 0.7051 - loss: 0.7964 - val_accuracy: 0.6369 - val_loss: 1.0264\n",
      "Epoch 35/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 479ms/step - accuracy: 0.7063 - loss: 0.7857 - val_accuracy: 0.6522 - val_loss: 0.9664\n",
      "Epoch 36/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 482ms/step - accuracy: 0.7066 - loss: 0.7733 - val_accuracy: 0.6627 - val_loss: 0.9458\n",
      "Epoch 37/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 482ms/step - accuracy: 0.7064 - loss: 0.7872 - val_accuracy: 0.6683 - val_loss: 0.9734\n",
      "Epoch 38/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 482ms/step - accuracy: 0.7149 - loss: 0.7650 - val_accuracy: 0.6641 - val_loss: 0.9565\n",
      "Epoch 39/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 479ms/step - accuracy: 0.7138 - loss: 0.7696 - val_accuracy: 0.6718 - val_loss: 0.9526\n",
      "Epoch 40/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 478ms/step - accuracy: 0.7189 - loss: 0.7577 - val_accuracy: 0.6557 - val_loss: 0.9766\n",
      "Epoch 41/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 480ms/step - accuracy: 0.7208 - loss: 0.7448 - val_accuracy: 0.6592 - val_loss: 0.9620\n",
      "Epoch 42/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 478ms/step - accuracy: 0.7261 - loss: 0.7365 - val_accuracy: 0.6683 - val_loss: 0.9830\n",
      "Epoch 43/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 468ms/step - accuracy: 0.7285 - loss: 0.7356 - val_accuracy: 0.6683 - val_loss: 0.9724\n",
      "Epoch 44/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 396ms/step - accuracy: 0.7294 - loss: 0.7175 - val_accuracy: 0.6578 - val_loss: 0.9670\n",
      "Epoch 45/45\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 398ms/step - accuracy: 0.7312 - loss: 0.7205 - val_accuracy: 0.6830 - val_loss: 0.9568\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = train_generator,epochs = epochs,validation_data = validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e39b0983-1e42-4ab2-8b7a-54862e88c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('emotions_models.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a79fad52-a9d2-49ba-8c83-25c1dfb29868",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluasi pada data test\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_generator)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Confusion Matrix dan Classification Report\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_generator' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluasi pada data test\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confusion Matrix dan Classification Report\n",
    "y_pred = np.argmax(model.predict(test_generator), axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9660c08-aa03-4129-a7b9-45b9e0c286e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Folder dataset\n",
    "train_dir = 'DatasetEmotion/train'  # Path ke folder train\n",
    "test_dir = 'DatasetEmotion/test'    # Path ke folder test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "245f17fe-d2fc-4d74-84be-bd237112fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing Data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4a6c4f-6259-4777-954c-00b15eaf232b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b5fbd-d791-43b9-8a0e-a37e34b81630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
